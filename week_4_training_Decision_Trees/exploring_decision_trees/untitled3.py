# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LtS9J9B_rgn1Z6Hwh9oS7ORBE1SWqKBK

# App Recommendation
"""

import pandas
import numpy
from matplotlib import pyplot as plt
import sklearn.tree

numpy.random.seed(0)

def process_app_one_hot(dataframe):
    """
    Bins age into Young, Adult
    Converts to categorical variables
    """

    column_age = "Age"
    bins_age = [0, 18, float("inf")] # Adult in U.S.
    labels_age = ['Young', 'Adult'] # labels must be one larger

    dataframe_p = dataframe.copy()
    # Bins age into Young, Adult
    dataframe_p[column_age] = pandas.cut(dataframe[column_age], bins_age, labels = labels_age) # labels keyword necessary
    # Converts to categorical variables
    dataframe_p = pandas.get_dummies(dataframe_p, prefix_sep='_') # defaults to all columns

    return dataframe_p

def process_app(dataframe):
    """
    Keeps age numeric
    Converts Platform to categorical
    Keeps App intact
    """
    columns_bin = ["Platform"]

    dataframe_p = dataframe.copy()
    dataframe_p = pandas.get_dummies(dataframe, columns=columns_bin, prefix_sep='_')

    return dataframe_p

dict_data = {
    'Platform': ['iPhone','iPhone','Android','iPhone','Android','Android'],
    'Age': [15, 25, 32, 35, 12, 14],
    'App': ['Atom Count', 'Check Mate Mate', 'Beehive Finder', 'Check Mate Mate', 'Atom Count', 'Atom Count']
    }

data_app = pandas.DataFrame(dict_data)

data_app

"""## With all categorical features"""

data_app_one_hot = process_app_one_hot(data_app)

columns_features = ['Platform_iPhone','Platform_Android','Age_Adult','Age_Young']
columns_labels = ['App_Atom Count','App_Beehive Finder','App_Check Mate Mate']

X = data_app_one_hot[columns_features]
y = data_app_one_hot[columns_labels]

data_app_one_hot

model_categorical = sklearn.tree.DecisionTreeClassifier()
model_categorical.fit(X,y)
model_categorical.score(X,y)

sklearn.tree.plot_tree(model_categorical, feature_names = columns_features, filled=True, rounded=True)

"""## With age as a numerical feature"""

data_app_p = process_app(data_app)

columns_features = ['Age','Platform_iPhone','Platform_Android']
features = data_app_p[columns_features].values
labels = data_app_p['App'].values

data_app_p

model_app = sklearn.tree.DecisionTreeClassifier()
model_app.fit(features, labels)
model_app.score(features, labels)

sklearn.tree.plot_tree(model_app, feature_names=columns_features, filled=True, rounded=True )

"""---

---

---

---

# About the dataset

Add Suggestion
Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:

CAR car acceptability
- . PRICE overall price
- . . buying buying price
- . . maint price of the maintenance
- . TECH technical characteristics
- . . COMFORT comfort
- . . . doors number of doors
- . . . persons capacity in terms of persons to carry
- . . . lug_boot the size of luggage boot
- . . safety estimated safety of the car

---

The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.

Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.

---
Attribute Information:

Class Values:

unacc, acc, good, vgood

Attributes:

buying: vhigh, high, med, low.
maint: vhigh, high, med, low.
doors: 2, 3, 4, 5more.
persons: 2, 4, more.
lug_boot: small, med, big.
safety: low, med, high.

---
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

col_names = ['buying_price', 'maintenance_cost', 'no_of_doors', 'no_of_persons', 'lug_boot', 'safety', 'decision']

df = pd.read_csv('car_evaluation.csv', skiprows=1, names=col_names)
df.head()

df.shape

df.info()

for col in df.columns:
    print(df[col].value_counts())

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

df['buying_price'] = encoder.fit_transform(df['buying_price'])
print(f"{['buying_price']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df['maintenance_cost'] = encoder.fit_transform(df['maintenance_cost'])
print(f"{['maintenance_cost']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df['no_of_doors'] = encoder.fit_transform(df['no_of_doors'])
print(f"{['no_of_doors']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df['no_of_persons'] = encoder.fit_transform(df['no_of_persons'])
print(f"{['no_of_persons']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df['lug_boot'] = encoder.fit_transform(df['lug_boot'])
print(f"{['lug_boot']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df['safety'] = encoder.fit_transform(df['safety'])
print(f"{['safety']} {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}")

df.head()

# separating the dataset to features and target
X = df.drop(['decision'], axis=1)
y = df['decision']

# split X and y into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

X_train.shape, X_test.shape

"""# 1. DecisionTreeClassifier with Entropy index


"""

from sklearn.tree import DecisionTreeClassifier

clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0)

clf_entropy.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred_gini = clf_entropy.predict(X_test)
y_pred_train_gini = clf_entropy.predict(X_train)

accuracy_test_data = accuracy_score(y_test, y_pred_gini)
print(f"Accuracy on test data: {accuracy_test_data}")
print('Test set score: {:.4f}'.format(clf_entropy.score(X_test, y_test)))

accuracy_train_data = accuracy_score(y_train, y_pred_train_gini)
print(f"Accuracy on train data: {accuracy_train_data}")
print('Training set score: {:.4f}'.format(clf_entropy.score(X_train, y_train)))

"""### Visulaize decision tree (Entropy)"""

from sklearn import tree


# Convert y_train to a list of strings
class_names_list = [str(cls) for cls in np.unique(y_train)]

plt.figure(figsize=(12,10))

# Instead of class_names_list, used X_train.columns to get the actual feature names
tree.plot_tree(clf_entropy.fit(X_train, y_train),
               feature_names=X_train.columns,
               class_names=class_names_list,
               filled=True,
               rounded=True)
plt.show()

"""# 2. DecisionTreeClassifier with Gini index"""

from sklearn.tree import DecisionTreeClassifier


clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=0)

clf_gini.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred_gini = clf_gini.predict(X_test)
y_pred_train_gini = clf_gini.predict(X_train)

accuracy_test_data = accuracy_score(y_test, y_pred_gini)
print(f"Accuracy on test data: {accuracy_test_data}")
print('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))

accuracy_train_data = accuracy_score(y_train, y_pred_train_gini)
print(f"Accuracy on train data: {accuracy_train_data}")
print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))

"""### Visualize decision tree (Gini Index)"""

from sklearn import tree


# Convert y_train to a list of strings
class_names_list = [str(cls) for cls in np.unique(y_train)]

plt.figure(figsize=(12,10))

# Instead of class_names_list, used X_train.columns to get the actual feature names
tree.plot_tree(clf_entropy.fit(X_train, y_train),
               feature_names=X_train.columns,
               class_names=class_names_list,
               filled=True,
               rounded=True)
plt.show()

"""---

---

---

# Implementon of Decison Tree Algorithm (used on Iris dataset)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy

import kagglehub

# Download latest version
path = kagglehub.dataset_download("uciml/iris")

print("Path to dataset files:", path)

mv {path} /content/sample_data

import pandas as pd
import numpy as np

df = pd.read_csv('/content/sample_data/Iris.csv')

print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")
df.head()

"""# Work on the model

# Node class
"""

class Node():
  def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):
    ''' constructor '''

    # for decision node
    self.feature_index = feature_index
    self.threshold = threshold
    self.left = left
    self.right = right
    self.info_gain = info_gain

    # for leaf node
    self.value = value

"""# Tree Class"""

class DecisionTreeClassifier():
  def __init__(self, min_samples_split=2, max_depth=2):
    ''' constructor '''
    # initialize the root of the tree
    self.root = None

    # stopping conditions
    self.min_samples_split = min_samples_split
    self.max_depth = max_depth


  def build_tree(self, dataset, curr_depth=0):
    ''' recursive function to build the tree '''
    X, Y = dataset[:,:-1], dataset[:,-1]
    num_samples, num_features = np.shape(X)

    # split untill stopping conditions are met
    if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:
      # find the best split
      best_split = self.get_best_split(dataset, num_samples, num_features)
      # check if information gain is positive
      if best_split["info_gain"] > 0:
        # recur left
        left_subtree = self.build_tree(best_split["dataset_left"], curr_depth+1)
        # recur right
        right_subtree = self.build_tree(best_split["dataset_right"], curr_depth+1)
        # return decision node
        return Node(best_split["feature_index"], best_split["threshold"],
                    left_subtree, right_subtree, best_split["info_gain"])

    # compute leaf node
    leaf_value = self.calculate_leaf_value(Y)
    # return leaf node
    return Node(value=leaf_value)


  def get_best_split(self, dataset, num_samples, num_features):
    ''' function to find the best split '''
    # Dictionary to store the best split
    best_split = {}
    max_info_gain = -float("inf")

    # Loop over all the features
    for feature_index in range(num_features):
      feature_values = dataset[:, feature_index]
      possible_thresholds = np.unique(feature_values)
      # Loop over all the feature values present in the data
      for threshold in possible_thresholds:
        # Get current split
        dataset_left, dataset_right = self.split(dataset, feature_index, threshold)
        # Check if childs are not null
        if len(dataset_left) > 0 and len(dataset_right) > 0:
          y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]
          # Compute information gain
          curr_info_gain = self.information_gain(y, left_y, right_y, "gini")
          # Update the best split if needed
          if curr_info_gain > max_info_gain:
            best_split["feature_index"] = feature_index
            best_split["threshold"] = threshold
            best_split["dataset_left"] = dataset_left
            best_split["dataset_right"] = dataset_right
            best_split["info_gain"] = curr_info_gain
            max_info_gain = curr_info_gain

    # Return best split
    return best_split


  def split(self, dataset, feature_index, threshold):
    ''' function to split the data '''
    dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])
    dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])
    return dataset_left, dataset_right


  def information_gain(self, parent, l_child, r_child, mode="entropy"):
    ''' function to compute information gain '''
    weight_l = len(l_child) / len(parent)
    weight_r = len(r_child) / len(parent)
    if mode == "gini":
      gain = self.gini_index(parent) - (weight_l * self.gini_index(l_child) + weight_r * self.gini_index(r_child))
    else:
      gain = self.entropy(parent) - (weight_l * self.entropy(l_child) + weight_r * self.entropy(r_child))
    return gain


  def entropy(self, y):
    ''' function to compute entropy '''
    class_labels = np.unique(y)
    entropy = 0
    for cls in class_labels:
      p_cls = len(y[y == cls]) / len(y)
      entropy += -p_cls * np.log2(p_cls)
    return entropy


  def gini_index(self, y):
    ''' function to compute gini index '''
    class_labels = np.unique(y)
    gini = 0
    for cls in class_labels:
      p_cls = len(y[y == cls]) / len(y)
      gini += p_cls ** 2
    return 1 - gini


  def calculate_leaf_value(self, Y):
    ''' function to compute leaf node '''
    Y = list(Y)
    return max(Y, key=Y.count)


  def print_tree(self, tree=None, indent=" "):
    ''' function to print the tree '''
    if not tree:
      tree = self.root
    if tree.value is not None:
      print(tree.value)
    else:
      print("X_" + str(tree.feature_index), "<=", tree.threshold, "?", tree.info_gain)
      print("%sleft:" % (indent), end="")
      self.print_tree(tree.left, indent + indent)
      print("%sright:" % (indent), end="")
      self.print_tree(tree.right, indent + indent)


  def fit(self, X, Y):
      ''' function to train the tree '''
      dataset = np.concatenate((X, Y), axis=1)
      self.root = self.build_tree(dataset)


  def predict(self, X):
    ''' function to predict new dataset '''
    preditions = [self.make_prediction(x, self.root) for x in X]
    return preditions


  def make_prediction(self, x, tree):
    ''' function to predict a single data point '''
    if tree.value != None: return tree.value
    feature_val = x[tree.feature_index]
    if feature_val <= tree.threshold:
      return self.make_prediction(x, tree.left)
    else:
      return self.make_prediction(x, tree.right)

"""# Train-Test split"""

X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values.reshape(-1,1)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)

"""# Fit the model"""

classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)
classifier.fit(X_train, Y_train)
classifier.print_tree()

"""# Test the model"""

Y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy_score(Y_test, Y_pred)