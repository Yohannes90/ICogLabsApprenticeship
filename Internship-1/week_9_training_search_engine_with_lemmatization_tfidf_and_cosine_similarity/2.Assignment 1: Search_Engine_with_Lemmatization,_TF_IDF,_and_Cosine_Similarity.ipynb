{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "i_gej3kyIj_i"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7e4NVbyZUQ1",
        "outputId": "999ec52f-56c6-475c-938c-3b0ebda82117"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng') # For POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "m7aKjX-sZ7ZZ"
      },
      "outputs": [],
      "source": [
        "english_stopset = set(stopwords.words('english')).union(\n",
        "                  {\"things\", \"that's\", \"something\", \"take\", \"don't\", \"may\", \"want\", \"you're\",\n",
        "                   \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\",\n",
        "                   \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\",\n",
        "                   \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\",\n",
        "                                                                  \"verb\", \"verbs\"})\n",
        "english_stopset = list(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xAlA4br4aP-X"
      },
      "outputs": [],
      "source": [
        "docs = ['i loved you ethiopian, stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe',\n",
        "\n",
        "        'also, sometimes, the same words can have multiple different ‘lemma’s. So, based on the context it’s used, you should identify the \\\n",
        "        part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes \\\n",
        "        in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the \\\n",
        "        Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages',\n",
        "\n",
        "        'With more than  million people, ethiopia is the second most populous nation in Africa after Nigeria, and the fastest growing \\\n",
        "         economy in the region. However, it is also one of the poorest, with a per capita income',\n",
        "\n",
        "        'The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring\\\n",
        "         countries.ethiopia With a planned.',\n",
        "\n",
        "        'The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The \\\n",
        "         word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.',\n",
        "\n",
        "        'Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power\\\n",
        "         generation will start well before the completion']\n",
        "\n",
        "title = ['Two upgraded', 'Loved Turbine-Generators', 'Operation With Loved', 'National', 'Power Grid', 'Generator']\n",
        "\n",
        "keywords = ['two','non','loved','ethiopia','operation','grid','power','fight','survive']  #we can generate keywords from articls using 'spacy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA-d-Mg6CU1x"
      },
      "source": [
        "## Issue 1: Title Wasn't Being Preprocessed\n",
        "- Refactored the preprocessing logic into a reusable function `preprocess` to preprocess both docs and title, ensuring the code follows the DRY (Don't Repeat Yourself) principle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRW69YucbaPa",
        "outputId": "7ac4ad70-33b7-4b00-9bdb-6ad574a2c393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i loved you ethiopian stored elements in compress find sparse ethiopia is the greatest country in the world of nation at universe', 'also sometimes the same words can have multiple different lemma s so based on the context it s used you should identify the part of speech pos tag for the word in that specific context and extract the appropriate lemma examples of implementing this comes in the following sections countries ethiopia with a planned the name that the blue nile river loved took in ethiopia is derived from the geez word for great to imply its being the river of rivers the word abay still exists in ethiopia major languages', 'with more than million people ethiopia is the second most populous nation in africa after nigeria and the fastest growing economy in the region however it is also one of the poorest with a per capita income', 'the primary purpose of the dam ethiopia is electricity production to relieve ethiopia s acute energy shortage and for electricity export to neighboring countries ethiopia with a planned', 'the name that the blue nile river loved takes in ethiopia abay is derived from the geez blue loved word for great to imply its being the river of rivers the word abay still exists in ethiopia major languages to refer to anything or anyone considered to be superior', 'two non upgraded loved turbine generators with mw each are the first loveto go into operation with loved mw delivered to the national power grid this early power generation will start well before the completion']\n",
            "['two upgraded', 'loved turbine generators', 'operation with loved', 'national', 'power grid', 'generator']\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # Remove punctuation\n",
        "    text = re.sub(r'[0-9]', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)  # Remove extra spaces\n",
        "    return text.strip()\n",
        "\n",
        "documents_clean = [preprocess(doc) for doc in docs]\n",
        "documents_cleant = [preprocess(doc) for doc in title]\n",
        "\n",
        "print(documents_clean)\n",
        "print(documents_cleant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu5yLO42Draf"
      },
      "source": [
        "## Issue 2: Lemmatizer Was Limited\n",
        "- The lemmatizer was only working for a small subset of words (e.g., converting \"elements\" to \"element\") but failed for words like those ending in \"-ed.\" This issue arose because the lemmatizer lacked information about the word's part of speech (noun, verb, adjective, etc.).\n",
        "\n",
        "- Resolved the issue by referring to the given article about lemmatization: [Link to Article](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/), Lemmatization Examples in Python, which provided helpful guidance.\n",
        "\n",
        "- Imported required libraries for nltk.pos_tag and added a reusable function, `get_wordnet_pos`. This function maps a word’s part of speech (POS) tag, such as noun or verb, to a format understood by WordNetLemmatizer (e.g., 'n' for noun, 'v' for verb).\n",
        "\n",
        "- Updated the lemmatizer call to include the POS argument: `lemmer.lemmatize(word, get_wordnet_pos(word))`.\n",
        "\n",
        "- Both previous and after update outputs are printed below to outline the changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anA2g6ZQbzlF",
        "outputId": "ab2405e2-8f1e-47dc-d879-c8d6e6a041e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous: ['i loved you ethiopian stored elements in compress find sparse ethiopia is the greatest country in the world of nation at universe', 'also sometimes the same words can have multiple different lemma s so based on the context it s used you should identify the part of speech pos tag for the word in that specific context and extract the appropriate lemma examples of implementing this comes in the following sections countries ethiopia with a planned the name that the blue nile river loved took in ethiopia is derived from the geez word for great to imply its being the river of rivers the word abay still exists in ethiopia major languages', 'with more than million people ethiopia is the second most populous nation in africa after nigeria and the fastest growing economy in the region however it is also one of the poorest with a per capita income', 'the primary purpose of the dam ethiopia is electricity production to relieve ethiopia s acute energy shortage and for electricity export to neighboring countries ethiopia with a planned', 'the name that the blue nile river loved takes in ethiopia abay is derived from the geez blue loved word for great to imply its being the river of rivers the word abay still exists in ethiopia major languages to refer to anything or anyone considered to be superior', 'two non upgraded loved turbine generators with mw each are the first loveto go into operation with loved mw delivered to the national power grid this early power generation will start well before the completion']\n",
            "Updated: ['i love you ethiopian store element in compress find sparse ethiopia be the great country in the world of nation at universe', 'also sometimes the same word can have multiple different lemma s so base on the context it s use you should identify the part of speech po tag for the word in that specific context and extract the appropriate lemma example of implement this come in the follow section country ethiopia with a plan the name that the blue nile river love take in ethiopia be derive from the geez word for great to imply it be the river of river the word abay still exists in ethiopia major language', 'with more than million people ethiopia be the second most populous nation in africa after nigeria and the fast grow economy in the region however it be also one of the poorest with a per caput income', 'the primary purpose of the dam ethiopia be electricity production to relieve ethiopia s acute energy shortage and for electricity export to neighbor country ethiopia with a plan', 'the name that the blue nile river love take in ethiopia abay be derive from the geez blue love word for great to imply it be the river of river the word abay still exists in ethiopia major language to refer to anything or anyone consider to be superior', 'two non upgraded love turbine generator with mw each be the first loveto go into operation with love mw deliver to the national power grid this early power generation will start well before the completion']\n",
            "\n",
            "Previous: ['Two upgraded', 'Loved Turbine-Generators', 'Operation With Loved', 'National', 'Power Grid', 'Generator']\n",
            "Updated: ['two upgraded', 'love turbine generator', 'operation with love', 'national', 'power grid', 'generator']\n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "lemmer=WordNetLemmatizer()\n",
        "\n",
        "new_docs_prev=[' '.join([lemmer.lemmatize(documents_clean) for documents_clean in text.split(',')]) for text in documents_clean]  #Lemmatization the words/description\n",
        "new_docs=[' '.join([lemmer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()]) for text in documents_clean]  #Lemmatization the words/description\n",
        "print(f\"Previous: {new_docs_prev}\")\n",
        "print(f\"Updated: {new_docs}\\n\")\n",
        "\n",
        "titles = [' '.join([lemmer.lemmatize(title).strip() for title in text.split(' ')]) for text in title]   #Lemmatization the title\n",
        "titles = [' '.join([lemmer.lemmatize(word, get_wordnet_pos(word)) for word in title.split()]) for title in documents_cleant]   #Lemmatization the title\n",
        "print(f\"Previous: {title}\")\n",
        "print(f\"Updated: {titles}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "VE5xmvrpcAIW"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                              ngram_range=(1, 2),\n",
        "                              min_df=0.002,\n",
        "                              max_df=0.99,\n",
        "                              max_features=10000,\n",
        "                              lowercase=True,\n",
        "                              stop_words=english_stopset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "xmwJekircqbe"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(new_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6AmioMPdMTb",
        "outputId": "29336543-d506-466b-cb51-28dbbcd60b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     0         1         2         3         4    5\n",
            "0  0.0  0.083112  0.000000  0.000000  0.229908  0.0\n",
            "1  0.0  0.000000  0.000000  0.000000  0.140185  0.0\n",
            "2  0.0  0.083112  0.000000  0.000000  0.114954  0.0\n",
            "3  0.0  0.000000  0.000000  0.174451  0.000000  0.0\n",
            "4  0.0  0.000000  0.000000  0.174451  0.000000  0.0\n",
            "5  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "6  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "7  0.0  0.083112  0.137421  0.000000  0.000000  0.0\n",
            "8  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "9  0.0  0.101354  0.000000  0.000000  0.000000  0.0\n",
            "(224, 6)\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame(X.T.toarray())\n",
        "print(df.head(10))\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvqiS4r9T5mt"
      },
      "source": [
        "## Issue 3: It finaly works for \"love\" but stopped working for \"loved\"\n",
        "- The issue was resolved by using the `get_similar_articles` function with an improved lemmatization approach. Specifically, I applied `lemmer.lemmatize(lemma_ops, get_wordnet_pos(lemma_ops))` to dynamically determine the word type (e.g., verb, noun, etc.) and apply the appropriate lemmatization. This ensured both \"love\" and \"loved\" were treated correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "VF4hYZtpdTHX"
      },
      "outputs": [],
      "source": [
        "def get_similar_articles(q,t, df):\n",
        "  print(\"Done Searching. Full Result: \\n\")\n",
        "  print(\"searched items : \", q)\n",
        "  print(\"Article with the Highest Cosine Similarity Values: \")\n",
        "  search_rank ={}\n",
        "  top_results=5\n",
        "  q = [q]\n",
        "  t = [t]\n",
        "\n",
        "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
        "  q_vect = vectorizer.transform(t).toarray().reshape(df.shape[0],)\n",
        "  sim = {}\n",
        "  titl = {}\n",
        "\n",
        "  for i in range(len(new_docs)) and range(len(titles)):\n",
        "    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)  #Calculate the similarity\n",
        "    # Or we can use cosine)similarity library both are the same\n",
        "    titl[i] = np.dot(df.loc[:, i].values, q_vect) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vect)\n",
        "\n",
        "  sim_sorted = sorted(sim.items(),key=lambda x : x[1], reverse=True)[:min(len(sim), top_results)]\n",
        "  sim_sortedt = sorted(titl.items(),key=lambda x : x[1], reverse=True)[:min(len(titl), top_results)]\n",
        "\n",
        "\n",
        "  for i, v in sim_sorted and sim_sortedt:    # Print the articles and their similarity values\n",
        "    if v != 0.0:\n",
        "      print(\"Similaritas score: \", v)\n",
        "      zip(titles, new_docs)\n",
        "      print(titles[i])\n",
        "      print(new_docs[i])\n",
        "      print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjgEISvFNi9i",
        "outputId": "25dfeb7a-f528-42c9-bda0-392e58156848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Searching. Full Result: \n",
            "\n",
            "searched items :  love\n",
            "Article with the Highest Cosine Similarity Values: \n",
            "Similaritas score:  0.17053622712109406\n",
            "generator\n",
            "two non upgraded love turbine generator with mw each be the first loveto go into operation with love mw deliver to the national power grid this early power generation will start well before the completion\n",
            "\n",
            "\n",
            "Similaritas score:  0.16633214688113884\n",
            "power grid\n",
            "the name that the blue nile river love take in ethiopia abay be derive from the geez blue love word for great to imply it be the river of river the word abay still exists in ethiopia major language to refer to anything or anyone consider to be superior\n",
            "\n",
            "\n",
            "Similaritas score:  0.12578354491916732\n",
            "two upgraded\n",
            "i love you ethiopian store element in compress find sparse ethiopia be the great country in the world of nation at universe\n",
            "\n",
            "\n",
            "Similaritas score:  0.06012924359789893\n",
            "love turbine generator\n",
            "also sometimes the same word can have multiple different lemma s so base on the context it s use you should identify the part of speech po tag for the word in that specific context and extract the appropriate lemma example of implement this come in the follow section country ethiopia with a plan the name that the blue nile river love take in ethiopia be derive from the geez word for great to imply it be the river of river the word abay still exists in ethiopia major language\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lemma_ops = 'loved'\n",
        "list1 = nltk.word_tokenize(lemma_ops)\n",
        "q1 = ' '.join([lemmer.lemmatize(lemma_ops, get_wordnet_pos(lemma_ops)) for lemma_ops in list1])\n",
        "\n",
        "get_similar_articles(q1,q1, df)\n",
        "print('-'*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAIEPaKSQjav",
        "outputId": "4f2dd45f-b9aa-4c75-af53-4908097eedfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Searching. Full Result: \n",
            "\n",
            "searched items :  love\n",
            "Article with the Highest Cosine Similarity Values: \n",
            "Similaritas score:  0.17053622712109406\n",
            "generator\n",
            "two non upgraded love turbine generator with mw each be the first loveto go into operation with love mw deliver to the national power grid this early power generation will start well before the completion\n",
            "\n",
            "\n",
            "Similaritas score:  0.16633214688113884\n",
            "power grid\n",
            "the name that the blue nile river love take in ethiopia abay be derive from the geez blue love word for great to imply it be the river of river the word abay still exists in ethiopia major language to refer to anything or anyone consider to be superior\n",
            "\n",
            "\n",
            "Similaritas score:  0.12578354491916732\n",
            "two upgraded\n",
            "i love you ethiopian store element in compress find sparse ethiopia be the great country in the world of nation at universe\n",
            "\n",
            "\n",
            "Similaritas score:  0.06012924359789893\n",
            "love turbine generator\n",
            "also sometimes the same word can have multiple different lemma s so base on the context it s use you should identify the part of speech po tag for the word in that specific context and extract the appropriate lemma example of implement this come in the follow section country ethiopia with a plan the name that the blue nile river love take in ethiopia be derive from the geez word for great to imply it be the river of river the word abay still exists in ethiopia major language\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lemma_ops = 'love'\n",
        "list1 = nltk.word_tokenize(lemma_ops)\n",
        "q1 = ' '.join([lemmer.lemmatize(lemma_ops, get_wordnet_pos(lemma_ops)) for lemma_ops in list1])\n",
        "\n",
        "get_similar_articles(q1,q1, df)\n",
        "print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukF1rwMmVk5E",
        "outputId": "73fec072-5362-43c0-d112-519d3b750c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Searching. Full Result: \n",
            "\n",
            "searched items :  electrical production\n",
            "Article with the Highest Cosine Similarity Values: \n",
            "Similaritas score:  0.17445107707418728\n",
            "national\n",
            "the primary purpose of the dam ethiopia be electricity production to relieve ethiopia s acute energy shortage and for electricity export to neighbor country ethiopia with a plan\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lemma_ops = 'electrical productions'\n",
        "list1 = nltk.word_tokenize(lemma_ops)\n",
        "q1 = ' '.join([lemmer.lemmatize(lemma_ops, get_wordnet_pos(lemma_ops)) for lemma_ops in list1])\n",
        "\n",
        "get_similar_articles(q1,q1, df)\n",
        "print('-' * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnM25eGwWgzu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
